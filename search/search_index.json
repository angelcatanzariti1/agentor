{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Agentor Documentation","text":"<p>Agentor is an open-source framework that makes it easy to build Agentic systems with secure integrations across email, calendars, CRMs, and more.</p> <p>It lets you connect LLMs to tools \u2014 like email, calendar, CRMs, or any data stack.</p>"},{"location":"#features","title":"Features","text":"Feature Description Docs \ud83d\ude80 LiteMCP The only full FastAPI compatible MCP Server with decorator API Link \ud83e\uddbe A2A Protocol Multi-agent communication Link \u2601\ufe0f Fast Agent deployment One click (serverless) deployment Link \ud83d\udd10 Secure integrations Multi-tenancy and fine-grained authorization Link \ud83d\udd0d Tool Search API Reduced tool context bloat Link"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>The recommended method of installing <code>agentor</code> is with pip from PyPI.</p> <pre><code>pip install agentor\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from agentor import Agentor\n\n# Create an agent\nagent = Agentor(\n    name=\"My Agent\", model=\"gpt-4\", instructions=\"You are a helpful assistant\"\n)\n\n# Run the agent\nresponse = agent.run(\"Hello, how are you?\")\nprint(response)\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>Browse the API Reference to explore the complete documentation of all modules, classes, and functions in Agentor.</p>"},{"location":"#resources","title":"Resources","text":"<ul> <li>GitHub Repository</li> <li>Examples</li> <li>Discord Community</li> </ul>"},{"location":"#license","title":"License","text":"<p>Agentor is released under the Apache 2.0 License.</p>"},{"location":"mcp_context/","title":"MCP Context: Accessing Headers and Cookies in Tools","text":"<p>The MCP Context feature allows tool functions to access HTTP request-level data such as headers and cookies. This is useful for implementing authentication, session management, user preferences, and other request-specific behaviors.</p>"},{"location":"mcp_context/#overview","title":"Overview","text":"<p>The <code>Context</code> class provides a simple interface to access: - Headers: All HTTP headers from the incoming request - Cookies: All cookies from the incoming request</p>"},{"location":"mcp_context/#basic-usage","title":"Basic Usage","text":"<pre><code>from agentor.mcp import MCPAPIRouter, Context, get_context\nfrom fastapi import Depends\n\nmcp_router = MCPAPIRouter()\n\n@mcp_router.tool(description=\"Get weather with user context\")\ndef get_weather(location: str, ctx: Context = Depends(get_context)) -&gt; str:\n    \"\"\"Get current weather information with user context\"\"\"\n    # Access request headers\n    user_agent = ctx.headers.get(\"user-agent\", \"unknown\")\n    auth_header = ctx.headers.get(\"authorization\", \"no-auth\")\n\n    # Access cookies\n    session_id = ctx.cookies.get(\"session_id\", \"no-session\")\n    user_pref = ctx.cookies.get(\"weather_units\", \"fahrenheit\")\n\n    # Use context in your logic\n    temp_symbol = \"\u00b0F\" if user_pref == \"fahrenheit\" else \"\u00b0C\"\n\n    return f\"Weather in {location}: Sunny, 72{temp_symbol}\"\n</code></pre>"},{"location":"mcp_context/#using-annotated-type-syntax","title":"Using Annotated Type Syntax","text":"<p>You can also use Python's <code>Annotated</code> type syntax:</p> <pre><code>from typing import Annotated\nfrom agentor.mcp import MCPAPIRouter, Context, get_context\nfrom fastapi import Depends\n\nmcp_router = MCPAPIRouter()\n\n@mcp_router.tool()\ndef check_auth(\n    resource: str, \n    ctx: Annotated[Context, Depends(get_context)]\n) -&gt; str:\n    \"\"\"Check authorization for a resource\"\"\"\n    auth_header = ctx.headers.get(\"authorization\", \"no-auth\")\n    return f\"Accessing {resource} with {auth_header}\"\n</code></pre>"},{"location":"mcp_context/#context-fields","title":"Context Fields","text":""},{"location":"mcp_context/#headers","title":"Headers","text":"<ul> <li>Type: <code>Dict[str, str]</code></li> <li>Contains all HTTP headers from the request</li> <li>Header names are lowercase (e.g., <code>\"user-agent\"</code>, <code>\"authorization\"</code>)</li> <li>Use <code>.get()</code> method for safe access with defaults</li> </ul>"},{"location":"mcp_context/#cookies","title":"Cookies","text":"<ul> <li>Type: <code>Dict[str, str]</code></li> <li>Contains all cookies from the request</li> <li>Use <code>.get()</code> method for safe access with defaults</li> </ul>"},{"location":"mcp_context/#common-use-cases","title":"Common Use Cases","text":""},{"location":"mcp_context/#1-authentication","title":"1. Authentication","text":"<pre><code>@mcp_router.tool()\ndef secure_action(action: str, ctx: Context = Depends(get_context)) -&gt; str:\n    auth_token = ctx.headers.get(\"authorization\", \"\")\n    if not auth_token.startswith(\"Bearer \"):\n        return \"Error: Unauthorized\"\n    # Process authenticated action\n    return f\"Action {action} completed\"\n</code></pre>"},{"location":"mcp_context/#2-user-preferences","title":"2. User Preferences","text":"<pre><code>@mcp_router.tool()\ndef get_content(page: str, ctx: Context = Depends(get_context)) -&gt; str:\n    language = ctx.headers.get(\"accept-language\", \"en\")\n    theme = ctx.cookies.get(\"theme\", \"light\")\n    return f\"Content for {page} in {language} with {theme} theme\"\n</code></pre>"},{"location":"mcp_context/#3-session-management","title":"3. Session Management","text":"<pre><code>@mcp_router.tool()\ndef add_to_cart(item: str, ctx: Context = Depends(get_context)) -&gt; str:\n    session_id = ctx.cookies.get(\"session_id\")\n    if not session_id:\n        return \"Error: No session\"\n    # Add item to user's cart\n    return f\"Added {item} to cart (session: {session_id})\"\n</code></pre>"},{"location":"mcp_context/#4-analytics-and-logging","title":"4. Analytics and Logging","text":"<pre><code>@mcp_router.tool()\ndef search(query: str, ctx: Context = Depends(get_context)) -&gt; str:\n    user_agent = ctx.headers.get(\"user-agent\", \"unknown\")\n    referer = ctx.headers.get(\"referer\", \"direct\")\n    # Log search with context\n    logger.info(f\"Search: {query} from {user_agent} via {referer}\")\n    return f\"Results for {query}\"\n</code></pre>"},{"location":"mcp_context/#important-notes","title":"Important Notes","text":"<ol> <li> <p>Schema Generation: Context parameters are automatically excluded from the tool's input schema, as they are resolved via dependency injection.</p> </li> <li> <p>Optional Context: Tools can work with or without context. Only include the <code>ctx</code> parameter if your tool needs request data.</p> </li> <li> <p>Thread Safety: The context uses Python's <code>contextvars</code> which are thread-safe and work correctly in async environments.</p> </li> <li> <p>Empty Context: If a tool is called outside of a request context (e.g., during testing), <code>get_context()</code> returns a Context with empty headers and cookies dictionaries.</p> </li> </ol>"},{"location":"mcp_context/#complete-example","title":"Complete Example","text":"<p>See <code>examples/mcp_context_example.py</code> for a complete working example that demonstrates: - Accessing headers and cookies - Using context in tool logic - Tools with and without context - Running the server</p>"},{"location":"mcp_context/#testing","title":"Testing","text":"<p>To test tools that use context, use FastAPI's TestClient:</p> <pre><code>from fastapi.testclient import TestClient\nfrom fastapi import FastAPI\n\napp = FastAPI()\napp.include_router(mcp_router.get_fastapi_router())\nclient = TestClient(app)\n\nresponse = client.post(\n    \"/mcp\",\n    json={\n        \"jsonrpc\": \"2.0\",\n        \"id\": 1,\n        \"method\": \"tools/call\",\n        \"params\": {\n            \"name\": \"get_weather\",\n            \"arguments\": {\"location\": \"NYC\"}\n        }\n    },\n    headers={\"user-agent\": \"test-client/1.0\"},\n    cookies={\"session_id\": \"test-123\"}\n)\n</code></pre> <p>See <code>tests/test_mcp_context.py</code> for comprehensive test examples.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the Agentor API Reference. This section provides detailed documentation for all modules, classes, and functions in the Agentor framework.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<p>The core modules provide the fundamental building blocks for creating and managing AI agents:</p> <ul> <li>Agent - Main agent implementation and orchestration</li> <li>LLM - Language model integration and management</li> <li>Schema - Data structures and schemas</li> <li>Tool Convertor - Tool conversion utilities</li> </ul>"},{"location":"api/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>The MCP modules enable building FastAPI-compatible MCP servers:</p> <ul> <li>API Router - MCP API routing with decorator support</li> <li>Server - MCP server implementation</li> <li>Proxy - MCP proxy functionality</li> </ul>"},{"location":"api/#tools","title":"Tools","text":"<p>The tools modules provide extensible tool registration and implementations:</p> <ul> <li>Registry - Tool registration system</li> <li>Base - Base classes for tools</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<p>Helper modules for various functionalities:</p> <ul> <li>Type Helper - Type definitions and helpers</li> <li>Output Text Formatter - Text formatting utilities</li> <li>Utils - General utility functions</li> </ul>"},{"location":"api/core/agent/","title":"Agent","text":""},{"location":"api/core/agent/#agentor.core.agent","title":"<code>agent</code>","text":""},{"location":"api/core/agent/#agentor.core.agent-classes","title":"Classes","text":""},{"location":"api/core/agent/#agentor.core.agent.Agentor","title":"<code>Agentor</code>","text":"<p>               Bases: <code>AgentorBase</code></p> <p>Build an Agent, connect tools, and serve as an API in just few lines of code.</p> Example <p>from agentor import Agentor agent = Agentor(name=\"Assistant\", instructions=\"You are a helpful assistant\") result = agent.run(\"Write a haiku about recursion in programming.\") print(result)</p> <p>Use any model supported by LiteLLM, e.g. \"gemini/gemini-pro\" or \"anthropic/claude-4\".     &gt;&gt;&gt; agent = Agentor(name=\"Assistant\", model=\"gemini/gemini-pro\", api_key=os.environ.get(\"GEMINI_API_KEY\"))</p> <p>Set model settings to configure the model behavior, e.g. temperature, top_p, etc.     &gt;&gt;&gt; from agentor import ModelSettings     &gt;&gt;&gt; model_settings = ModelSettings(temperature=0.5)     &gt;&gt;&gt; agent = Agentor(name=\"Assistant\", model=\"gemini/gemini-pro\", api_key=os.environ.get(\"GEMINI_API_KEY\"), model_settings=model_settings)</p> Source code in <code>src/agentor/core/agent.py</code> <pre><code>class Agentor(AgentorBase):\n    \"\"\"\n    Build an Agent, connect tools, and serve as an API in just few lines of code.\n\n    Example:\n        &gt;&gt;&gt; from agentor import Agentor\n        &gt;&gt;&gt; agent = Agentor(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n        &gt;&gt;&gt; result = agent.run(\"Write a haiku about recursion in programming.\")\n        &gt;&gt;&gt; print(result)\n\n        &gt;&gt;&gt; # Serve the Agent as an API\n        &gt;&gt;&gt; agent.serve(port=8000)\n\n    Use any model supported by LiteLLM, e.g. \"gemini/gemini-pro\" or \"anthropic/claude-4\".\n        &gt;&gt;&gt; agent = Agentor(name=\"Assistant\", model=\"gemini/gemini-pro\", api_key=os.environ.get(\"GEMINI_API_KEY\"))\n\n    Set model settings to configure the model behavior, e.g. temperature, top_p, etc.\n        &gt;&gt;&gt; from agentor import ModelSettings\n        &gt;&gt;&gt; model_settings = ModelSettings(temperature=0.5)\n        &gt;&gt;&gt; agent = Agentor(name=\"Assistant\", model=\"gemini/gemini-pro\", api_key=os.environ.get(\"GEMINI_API_KEY\"), model_settings=model_settings)\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        instructions: Optional[str] = None,\n        model: Optional[str | LitellmModel] = \"gpt-5-nano\",\n        tools: Optional[\n            List[\n                Union[\n                    FunctionTool,\n                    str,\n                    MCPServerStreamableHttp,\n                    BaseTool,\n                ]\n            ]\n        ] = None,\n        output_type: type[Any] | AgentOutputSchemaBase | None = None,\n        debug: bool = False,\n        api_key: Optional[str] = None,\n        model_settings: Optional[ModelSettings] = None,\n        skills: Optional[List[str]] = None,\n        enable_tracing: bool = False,\n    ):\n        if skills is not None:\n            available_skills = self._inject_skills(skills)\n            instructions = f\"{instructions or ''}\\n\\n{available_skills}\"\n        super().__init__(name, instructions, model, api_key, enable_tracing)\n        tools = tools or []\n        resolved_tools: List[FunctionTool] = []\n        mcp_servers: List[MCPServerStreamableHttp] = []\n\n        for tool in tools:\n            if isinstance(tool, str):\n                resolved_tools.append(ToolRegistry.get(tool)[\"tool\"])\n            elif isinstance(tool, FunctionTool):\n                resolved_tools.append(tool)\n            elif isinstance(tool, BaseTool):\n                # Convert all capabilities to individual OpenAI functions\n                resolved_tools.extend(tool.to_openai_function())\n            elif isinstance(tool, MCPServerStreamableHttp):\n                mcp_servers.append(tool)\n            elif isinstance(tool, WebSearchTool):\n                resolved_tools.append(tool)\n            else:\n                raise TypeError(\n                    f\"Unsupported tool type '{type(tool).__name__}'. \"\n                    \"Expected str, FunctionTool, ToolConvertor, BaseTool, or MCPServerStreamableHttp.\"\n                )\n\n        self.tools = resolved_tools\n        self.mcp_servers = mcp_servers\n\n        if model_settings is None:\n            model_settings = get_default_model_settings()\n\n        if self.api_key:\n            set_default_openai_key(self.api_key)\n\n        self.agent: Agent = Agent(\n            name=name,\n            instructions=instructions,\n            model=self.model,\n            tools=self.tools,\n            mcp_servers=self.mcp_servers or [],\n            output_type=output_type,\n            model_settings=model_settings,\n        )\n\n    def _inject_skills(self, skills: List[str]) -&gt; str:\n        \"\"\"Inject skills into the agent system prompt.\"\"\"\n        instructions = []\n        for skill in skills:\n            skill = Skills.load_from_path(skill)\n            instructions.append(f\"{skill.to_xml()}\")\n        return \"&lt;available_skills&gt;\" + \"\".join(instructions) + \"&lt;/available_skills&gt;\"\n\n    @classmethod\n    def from_md(\n        cls,\n        md_path: str | Path,\n        *,\n        model: Optional[str | LitellmModel] = None,\n        tools: Optional[\n            List[\n                Union[\n                    FunctionTool,\n                    str,\n                    MCPServerStreamableHttp,\n                    BaseTool,\n                ]\n            ]\n        ] = None,\n        output_type: type[Any] | AgentOutputSchemaBase | None = None,\n        debug: bool = False,\n        api_key: Optional[str] = None,\n        model_settings: Optional[ModelSettings] = None,\n    ) -&gt; \"Agentor\":\n        \"\"\"\n        Create an Agentor instance from a markdown file.\n\n        Expected markdown structure:\n\n            ---\n            name: Agent name\n            tools: [\"get_weather\", \"gmail\"]  # or as a string: \"get_weather, gmail\"\n            model: gpt-4o\n            temperature: 0.3\n            ---\n            System prompt goes here\n\n        The `tools` field is optional. Unknown tools are ignored for now to\n        keep the v0 experience simple.\n\n        Note: If `model_settings` is provided without a temperature, the temperature\n        from the markdown frontmatter will be merged into it.\n        \"\"\"\n        path = Path(md_path)\n        if not path.is_file():\n            raise FileNotFoundError(f\"Markdown file not found: {path}\")\n\n        post = frontmatter.loads(path.read_text(encoding=\"utf-8\"))\n        metadata = {key.lower(): value for key, value in (post.metadata or {}).items()}\n\n        name = metadata.get(\"name\")\n        if not name:\n            raise ValueError(\"Agent name is required in the markdown frontmatter.\")\n\n        instructions = post.content.strip()\n        if not instructions:\n            raise ValueError(\"Agent instructions are required in the markdown body.\")\n\n        temperature = metadata.get(\"temperature\")\n        parsed_temperature: Optional[float] = None\n        if temperature is not None:\n            try:\n                parsed_temperature = float(temperature)\n            except (TypeError, ValueError):\n                raise ValueError(\n                    \"Temperature in markdown frontmatter must be a number.\"\n                )\n\n        resolved_tools: Optional[\n            List[\n                Union[\n                    FunctionTool,\n                    str,\n                    MCPServerStreamableHttp,\n                    BaseTool,\n                ]\n            ]\n        ]\n        if tools is not None:\n            resolved_tools = tools\n        else:\n            tool_names = metadata.get(\"tools\")\n            if tool_names:\n                if isinstance(tool_names, str):\n                    parsed_tools = [item.strip() for item in tool_names.split(\",\")]\n                elif isinstance(tool_names, (list, tuple)):\n                    parsed_tools = [str(item).strip() for item in tool_names]\n                else:\n                    raise ValueError(\n                        \"Tools in markdown frontmatter must be a string or a list.\"\n                    )\n                available_tools = set(ToolRegistry.list())\n                unknown_tools = [\n                    tool_name\n                    for tool_name in parsed_tools\n                    if tool_name and tool_name not in available_tools\n                ]\n                if unknown_tools:\n                    logger.warning(\n                        \"Ignoring unknown tools in %s: %s\",\n                        path,\n                        \", \".join(unknown_tools),\n                    )\n                resolved_tools = [\n                    tool_name\n                    for tool_name in parsed_tools\n                    if tool_name and tool_name in available_tools\n                ] or None\n            else:\n                resolved_tools = None\n\n        resolved_model_settings = model_settings\n        if parsed_temperature is not None:\n            if resolved_model_settings is None:\n                resolved_model_settings = ModelSettings(temperature=parsed_temperature)\n            elif getattr(resolved_model_settings, \"temperature\", None) is None:\n                # Merge temperature from markdown into provided model_settings\n                settings_dict = dataclasses.asdict(resolved_model_settings)\n                settings_dict[\"temperature\"] = parsed_temperature\n                resolved_model_settings = ModelSettings(**settings_dict)\n\n        metadata_model = metadata.get(\"model\")\n        resolved_model = model or metadata_model or \"gpt-5-nano\"\n\n        return cls(\n            name=name,\n            instructions=instructions,\n            model=resolved_model,\n            tools=resolved_tools,\n            output_type=output_type,\n            debug=debug,\n            api_key=api_key,\n            model_settings=resolved_model_settings,\n        )\n\n    def run(self, input: str) -&gt; List[str] | str:\n        return Runner.run_sync(self.agent, input, context=CelestoConfig())\n\n    async def arun(\n        self,\n        input: list[str] | str | list[AgentInputType],\n        limit_concurrency: int = 10,\n        max_turns: int = 20,\n        fallback_models: Optional[List[str]] = None,\n    ) -&gt; List[str] | str:\n        \"\"\"\n        Run the agent with an input prompt or a batch of prompts.\n        In case of a batch of prompts, the agent will run each prompt concurrently.\n\n        Args:\n            input: A string prompt or a list of string prompts.\n            limit_concurrency: The maximum number of concurrent tasks to run in case of a batch of prompts.\n            max_turns: The maximum number of turns to run the agent.\n            fallback_models: Optional list of fallback model names to try if the primary model\n                fails due to rate limits or API errors. Models are tried in order.\n        \"\"\"\n        if isinstance(input, list):\n            if isinstance(input[0], dict):\n                return await Runner.run(self.agent, input, context=CelestoConfig())\n\n            futures = []\n            if limit_concurrency &gt; 0:\n                semaphore = asyncio.Semaphore(limit_concurrency)\n\n                async def _run_task(task: str) -&gt; str:\n                    async with semaphore:\n                        return await self._run_with_fallback(\n                            task, max_turns, fallback_models\n                        )\n\n                futures = [_run_task(task) for task in input]\n                return await asyncio.gather(*futures, return_exceptions=True)\n            else:\n                return await asyncio.gather(\n                    *[\n                        self._run_with_fallback(task, max_turns, fallback_models)\n                        for task in input\n                    ],\n                    return_exceptions=True,\n                )\n        else:\n            return await self._run_with_fallback(input, max_turns, fallback_models)\n\n    async def _run_with_fallback(\n        self,\n        task: str,\n        max_turns: int,\n        fallback_models: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Run a task with optional fallback to alternative models on rate limit errors.\n        \"\"\"\n        try:\n            return await Runner.run(\n                self.agent,\n                task,\n                context=CelestoConfig(),\n                max_turns=max_turns,\n            )\n        except (\n            openai.RateLimitError,\n            litellm.RateLimitError,\n            openai.APIError,\n            litellm.APIError,\n        ) as e:\n            if not fallback_models:\n                raise\n\n            logger.warning(\n                f\"Primary model failed with {type(e).__name__}: {e}. \"\n                f\"Trying fallback models: {fallback_models}\"\n            )\n\n            for fallback_model in fallback_models:\n                try:\n                    # Create a temporary agent with the fallback model\n                    fallback_agent = Agent(\n                        name=self.agent.name,\n                        instructions=self.agent.instructions,\n                        model=LitellmModel(fallback_model)\n                        if \"/\" in fallback_model\n                        else fallback_model,\n                        tools=self.tools,\n                        mcp_servers=self.mcp_servers or [],\n                        output_type=self.agent.output_type,\n                        model_settings=self.agent.model_settings,\n                    )\n                    return await Runner.run(\n                        fallback_agent,\n                        task,\n                        context=CelestoConfig(),\n                        max_turns=max_turns,\n                    )\n                except (\n                    openai.RateLimitError,\n                    litellm.RateLimitError,\n                    openai.APIError,\n                    litellm.APIError,\n                ) as fallback_error:\n                    logger.warning(\n                        f\"Fallback model '{fallback_model}' also failed: {fallback_error}\"\n                    )\n                    continue\n\n            # All fallback models failed, raise the original error\n            raise\n\n    def think(self, query: str) -&gt; List[str] | str:\n        prompt = render_prompt(\n            THINKING_PROMPT,\n            query=query,\n        )\n        result = Runner.run_sync(self.agent, prompt, context=CelestoConfig())\n        return result.final_output\n\n    async def chat(\n        self,\n        input: str,\n        stream: bool = False,\n        serialize: bool = True,\n    ):\n        if stream:\n            return self.stream_chat(input, serialize=serialize)\n        else:\n            return await Runner.run(self.agent, input=input, context=CelestoConfig())\n\n    async def stream_chat(\n        self,\n        input: str,\n        serialize: bool = True,\n    ) -&gt; AsyncIterator[Union[str, AgentOutput]]:\n        result = Runner.run_streamed(self.agent, input=input, context=CelestoConfig())\n        async for agent_output in format_stream_events(\n            result.stream_events(),\n            allowed_events=[\"run_item_stream_event\"],\n        ):\n            if serialize:\n                yield agent_output.serialize(dump_json=True)\n            else:\n                yield agent_output\n\n    def serve(\n        self,\n        host: Literal[\"0.0.0.0\", \"127.0.0.1\", \"localhost\"] = \"0.0.0.0\",\n        port: int = 8000,\n        log_level: Literal[\"debug\", \"info\", \"warning\", \"error\"] = \"info\",\n        access_log: bool = True,\n    ):\n        if host not in (\"0.0.0.0\", \"127.0.0.1\", \"localhost\"):\n            raise ValueError(\n                f\"Invalid host: {host}. Must be 0.0.0.0, 127.0.0.1, or localhost.\"\n            )\n\n        app = self._create_app(host, port)\n        print(f\"Running Agentor at http://{host}:{port}\")\n        print(\n            f\"Agent card available at http://{host}:{port}/.well-known/agent-card.json\"\n        )\n        uvicorn.run(\n            app, host=host, port=port, log_level=log_level, access_log=access_log\n        )\n\n    def _create_app(self, host: str, port: int) -&gt; FastAPI:\n        skills = (\n            [\n                AgentSkill(\n                    id=f\"tool_{tool.name.lower().replace(' ', '_')}\",\n                    name=tool.name,\n                    description=tool.description,\n                    tags=[],\n                )\n                for tool in self.tools\n            ]\n            if self.tools\n            else []\n        )\n        controller = A2AController(\n            name=self.name,\n            description=self.instructions,\n            skills=skills,\n            url=f\"http://{host}:{port}\",\n        )\n        controller.add_api_route(\"/chat\", self._chat_handler, methods=[\"POST\"])\n        controller.add_api_route(\"/health\", self._health_check_handler, methods=[\"GET\"])\n\n        self._register_a2a_handlers(controller)\n\n        app = FastAPI()\n        app.include_router(controller)\n        return app\n\n    async def _chat_handler(self, data: APIInputRequest) -&gt; str:\n        if data.stream:\n            return StreamingResponse(\n                self.stream_chat(data.input, serialize=True),\n                media_type=\"text/event-stream\",\n            )\n        else:\n            result = await self.chat(data.input)\n            return result.final_output\n\n    async def _health_check_handler(self) -&gt; Response:\n        return Response(status_code=200, content=\"OK\")\n\n    def _register_a2a_handlers(self, controller: A2AController):\n        controller.add_handler(\"message/stream\", self._message_stream_handler)\n\n    async def _message_stream_handler(\n        self, request: a2a_types.SendStreamingMessageRequest\n    ) -&gt; StreamingResponse:\n        async def event_generator() -&gt; AsyncGenerator[str, None]:\n            task_id = f\"task_{uuid.uuid4()}\"\n            context_id = f\"ctx_{uuid.uuid4()}\"\n            artifact_id = f\"artifact_{uuid.uuid4()}\"\n\n            try:\n                # Send initial task\n                task = Task(\n                    id=task_id,\n                    context_id=context_id,\n                    status=TaskStatus(state=TaskState.working),\n                )\n                response = JSONRPCResponse(id=request.id, result=task.model_dump())\n                yield f\"data: {json.dumps(response.model_dump())}\\n\\n\"\n\n                # Extract message text\n                if (\n                    request.params.message.parts is None\n                    or len(request.params.message.parts) == 0\n                ):\n                    raise ValueError(\n                        f\"Message parts are required but got {request.params.message.parts}.\"\n                    )\n                part = request.params.message.parts[0].root\n                if part.kind != \"text\":\n                    raise ValueError(f\"Invalid part kind: {part.kind}. Must be 'text'.\")\n                input_text = part.text\n\n                # Stream artifact updates\n                result = self.stream_chat(input_text, serialize=False)\n                is_first_chunk = True\n\n                async for event in result:\n                    event: AgentOutput\n                    if event.message is not None:\n                        artifact = a2a_types.Artifact(\n                            artifact_id=artifact_id,\n                            name=\"response\",\n                            description=\"Agent response text\",\n                            parts=[\n                                a2a_types.Part(\n                                    root=a2a_types.TextPart(text=event.message)\n                                )\n                            ],\n                        )\n                        artifact_update = a2a_types.TaskArtifactUpdateEvent(\n                            kind=\"artifact-update\",\n                            task_id=task_id,\n                            context_id=context_id,\n                            artifact=artifact,\n                            append=not is_first_chunk,\n                        )\n                        response = JSONRPCResponse(\n                            id=request.id, result=artifact_update.model_dump()\n                        )\n                        yield f\"data: {json.dumps(response.model_dump())}\\n\\n\"\n                        is_first_chunk = False\n\n                # Send completion status\n                final_status = a2a_types.TaskStatusUpdateEvent(\n                    task_id=task_id,\n                    context_id=context_id,\n                    status=TaskStatus(state=TaskState.completed),\n                    final=True,\n                )\n                response = JSONRPCResponse(\n                    id=request.id, result=final_status.model_dump()\n                )\n                yield f\"data: {json.dumps(response.model_dump())}\\n\\n\"\n\n            except Exception as e:\n                logger.exception(f\"Error in A2A stream handler: {e}\")\n\n                error_status = a2a_types.TaskStatusUpdateEvent(\n                    task_id=task_id,\n                    context_id=context_id,\n                    status=TaskStatus(state=TaskState.failed, message=str(e)),\n                    final=True,\n                )\n                response = JSONRPCResponse(\n                    id=request.id, result=error_status.model_dump()\n                )\n                yield f\"data: {json.dumps(response.model_dump())}\\n\\n\"\n\n        return StreamingResponse(\n            event_generator(),\n            media_type=\"text/event-stream\",\n            headers={\n                \"Cache-Control\": \"no-cache\",\n                \"Connection\": \"keep-alive\",\n            },\n        )\n</code></pre>"},{"location":"api/core/agent/#agentor.core.agent.Agentor--serve-the-agent-as-an-api","title":"Serve the Agent as an API","text":"<p>agent.serve(port=8000)</p>"},{"location":"api/core/agent/#agentor.core.agent.Agentor-functions","title":"Functions","text":""},{"location":"api/core/agent/#agentor.core.agent.Agentor.from_md","title":"<code>from_md(md_path, *, model=None, tools=None, output_type=None, debug=False, api_key=None, model_settings=None)</code>  <code>classmethod</code>","text":"<p>Create an Agentor instance from a markdown file.</p> <p>Expected markdown structure:</p> <pre><code>---\nname: Agent name\ntools: [\"get_weather\", \"gmail\"]  # or as a string: \"get_weather, gmail\"\nmodel: gpt-4o\ntemperature: 0.3\n---\nSystem prompt goes here\n</code></pre> <p>The <code>tools</code> field is optional. Unknown tools are ignored for now to keep the v0 experience simple.</p> <p>Note: If <code>model_settings</code> is provided without a temperature, the temperature from the markdown frontmatter will be merged into it.</p> Source code in <code>src/agentor/core/agent.py</code> <pre><code>@classmethod\ndef from_md(\n    cls,\n    md_path: str | Path,\n    *,\n    model: Optional[str | LitellmModel] = None,\n    tools: Optional[\n        List[\n            Union[\n                FunctionTool,\n                str,\n                MCPServerStreamableHttp,\n                BaseTool,\n            ]\n        ]\n    ] = None,\n    output_type: type[Any] | AgentOutputSchemaBase | None = None,\n    debug: bool = False,\n    api_key: Optional[str] = None,\n    model_settings: Optional[ModelSettings] = None,\n) -&gt; \"Agentor\":\n    \"\"\"\n    Create an Agentor instance from a markdown file.\n\n    Expected markdown structure:\n\n        ---\n        name: Agent name\n        tools: [\"get_weather\", \"gmail\"]  # or as a string: \"get_weather, gmail\"\n        model: gpt-4o\n        temperature: 0.3\n        ---\n        System prompt goes here\n\n    The `tools` field is optional. Unknown tools are ignored for now to\n    keep the v0 experience simple.\n\n    Note: If `model_settings` is provided without a temperature, the temperature\n    from the markdown frontmatter will be merged into it.\n    \"\"\"\n    path = Path(md_path)\n    if not path.is_file():\n        raise FileNotFoundError(f\"Markdown file not found: {path}\")\n\n    post = frontmatter.loads(path.read_text(encoding=\"utf-8\"))\n    metadata = {key.lower(): value for key, value in (post.metadata or {}).items()}\n\n    name = metadata.get(\"name\")\n    if not name:\n        raise ValueError(\"Agent name is required in the markdown frontmatter.\")\n\n    instructions = post.content.strip()\n    if not instructions:\n        raise ValueError(\"Agent instructions are required in the markdown body.\")\n\n    temperature = metadata.get(\"temperature\")\n    parsed_temperature: Optional[float] = None\n    if temperature is not None:\n        try:\n            parsed_temperature = float(temperature)\n        except (TypeError, ValueError):\n            raise ValueError(\n                \"Temperature in markdown frontmatter must be a number.\"\n            )\n\n    resolved_tools: Optional[\n        List[\n            Union[\n                FunctionTool,\n                str,\n                MCPServerStreamableHttp,\n                BaseTool,\n            ]\n        ]\n    ]\n    if tools is not None:\n        resolved_tools = tools\n    else:\n        tool_names = metadata.get(\"tools\")\n        if tool_names:\n            if isinstance(tool_names, str):\n                parsed_tools = [item.strip() for item in tool_names.split(\",\")]\n            elif isinstance(tool_names, (list, tuple)):\n                parsed_tools = [str(item).strip() for item in tool_names]\n            else:\n                raise ValueError(\n                    \"Tools in markdown frontmatter must be a string or a list.\"\n                )\n            available_tools = set(ToolRegistry.list())\n            unknown_tools = [\n                tool_name\n                for tool_name in parsed_tools\n                if tool_name and tool_name not in available_tools\n            ]\n            if unknown_tools:\n                logger.warning(\n                    \"Ignoring unknown tools in %s: %s\",\n                    path,\n                    \", \".join(unknown_tools),\n                )\n            resolved_tools = [\n                tool_name\n                for tool_name in parsed_tools\n                if tool_name and tool_name in available_tools\n            ] or None\n        else:\n            resolved_tools = None\n\n    resolved_model_settings = model_settings\n    if parsed_temperature is not None:\n        if resolved_model_settings is None:\n            resolved_model_settings = ModelSettings(temperature=parsed_temperature)\n        elif getattr(resolved_model_settings, \"temperature\", None) is None:\n            # Merge temperature from markdown into provided model_settings\n            settings_dict = dataclasses.asdict(resolved_model_settings)\n            settings_dict[\"temperature\"] = parsed_temperature\n            resolved_model_settings = ModelSettings(**settings_dict)\n\n    metadata_model = metadata.get(\"model\")\n    resolved_model = model or metadata_model or \"gpt-5-nano\"\n\n    return cls(\n        name=name,\n        instructions=instructions,\n        model=resolved_model,\n        tools=resolved_tools,\n        output_type=output_type,\n        debug=debug,\n        api_key=api_key,\n        model_settings=resolved_model_settings,\n    )\n</code></pre>"},{"location":"api/core/agent/#agentor.core.agent.Agentor.arun","title":"<code>arun(input, limit_concurrency=10, max_turns=20, fallback_models=None)</code>  <code>async</code>","text":"<p>Run the agent with an input prompt or a batch of prompts. In case of a batch of prompts, the agent will run each prompt concurrently.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>list[str] | str | list[AgentInputType]</code> <p>A string prompt or a list of string prompts.</p> required <code>limit_concurrency</code> <code>int</code> <p>The maximum number of concurrent tasks to run in case of a batch of prompts.</p> <code>10</code> <code>max_turns</code> <code>int</code> <p>The maximum number of turns to run the agent.</p> <code>20</code> <code>fallback_models</code> <code>Optional[List[str]]</code> <p>Optional list of fallback model names to try if the primary model fails due to rate limits or API errors. Models are tried in order.</p> <code>None</code> Source code in <code>src/agentor/core/agent.py</code> <pre><code>async def arun(\n    self,\n    input: list[str] | str | list[AgentInputType],\n    limit_concurrency: int = 10,\n    max_turns: int = 20,\n    fallback_models: Optional[List[str]] = None,\n) -&gt; List[str] | str:\n    \"\"\"\n    Run the agent with an input prompt or a batch of prompts.\n    In case of a batch of prompts, the agent will run each prompt concurrently.\n\n    Args:\n        input: A string prompt or a list of string prompts.\n        limit_concurrency: The maximum number of concurrent tasks to run in case of a batch of prompts.\n        max_turns: The maximum number of turns to run the agent.\n        fallback_models: Optional list of fallback model names to try if the primary model\n            fails due to rate limits or API errors. Models are tried in order.\n    \"\"\"\n    if isinstance(input, list):\n        if isinstance(input[0], dict):\n            return await Runner.run(self.agent, input, context=CelestoConfig())\n\n        futures = []\n        if limit_concurrency &gt; 0:\n            semaphore = asyncio.Semaphore(limit_concurrency)\n\n            async def _run_task(task: str) -&gt; str:\n                async with semaphore:\n                    return await self._run_with_fallback(\n                        task, max_turns, fallback_models\n                    )\n\n            futures = [_run_task(task) for task in input]\n            return await asyncio.gather(*futures, return_exceptions=True)\n        else:\n            return await asyncio.gather(\n                *[\n                    self._run_with_fallback(task, max_turns, fallback_models)\n                    for task in input\n                ],\n                return_exceptions=True,\n            )\n    else:\n        return await self._run_with_fallback(input, max_turns, fallback_models)\n</code></pre>"},{"location":"api/core/agent/#agentor.core.agent-functions","title":"Functions","text":""},{"location":"api/core/agent/#agentor.core.agent.get_dummy_weather","title":"<code>get_dummy_weather(city)</code>","text":"<p>Returns the dummy weather in the given city.</p> Source code in <code>src/agentor/core/agent.py</code> <pre><code>@function_tool(name_override=\"get_weather\")\ndef get_dummy_weather(city: str) -&gt; str:\n    \"\"\"Returns the dummy weather in the given city.\"\"\"\n    return f\"The dummy weather in {city} is sunny\"\n</code></pre>"},{"location":"api/core/llm/","title":"LLM","text":""},{"location":"api/core/llm/#agentor.core.llm","title":"<code>llm</code>","text":""},{"location":"api/core/schema/","title":"Schema","text":""},{"location":"api/core/schema/#agentor.core.schema","title":"<code>schema</code>","text":""},{"location":"api/core/schema/#agentor.core.schema-classes","title":"Classes","text":""},{"location":"api/core/schema/#agentor.core.schema.JSONRPCErrorCodes","title":"<code>JSONRPCErrorCodes</code>  <code>dataclass</code>","text":"<p>JSON-RPC 2.0 error codes as defined in the specification. Organized into standard error codes and custom server error codes.</p> Source code in <code>src/agentor/core/schema.py</code> <pre><code>@dataclass(frozen=True)\nclass JSONRPCErrorCodes:\n    \"\"\"\n    JSON-RPC 2.0 error codes as defined in the specification.\n    Organized into standard error codes and custom server error codes.\n    \"\"\"\n\n    # Standard JSON-RPC 2.0 Error Codes\n    PARSE_ERROR: int = -32700  # Invalid JSON was received by the server\n    INVALID_REQUEST: int = -32600  # The JSON sent is not a valid Request object\n    METHOD_NOT_FOUND: int = -32601  # The method does not exist / is not available\n    INVALID_PARAMS: int = -32602  # Invalid method parameter(s)\n    INTERNAL_ERROR: int = -32603  # Internal JSON-RPC error\n\n    # Server Error Codes (custom implementation-defined errors: -32000 to -32099)\n    SERVER_ERROR_NOT_IMPLEMENTED: int = (\n        -32000\n    )  # Method exists but is not implemented yet\n    SERVER_ERROR_UNAUTHORIZED: int = -32001  # Authentication required or failed\n    SERVER_ERROR_FORBIDDEN: int = -32002  # Authenticated but not authorized\n    SERVER_ERROR_RESOURCE_NOT_FOUND: int = -32003  # Requested resource not found\n    SERVER_ERROR_TIMEOUT: int = -32004  # Operation timed out\n</code></pre>"},{"location":"api/mcp/api_router/","title":"API Router","text":""},{"location":"api/mcp/api_router/#agentor.mcp.api_router","title":"<code>api_router</code>","text":""},{"location":"api/mcp/api_router/#agentor.mcp.api_router-classes","title":"Classes","text":""},{"location":"api/mcp/api_router/#agentor.mcp.api_router.Context","title":"<code>Context</code>  <code>dataclass</code>","text":"<p>Context object providing access to request-level data in MCP tools</p> <p>This class provides access to HTTP headers and cookies from the incoming request. Use it as a dependency in your tool functions to access request context.</p> Example <p>@mcp_router.tool() def my_tool(location: str, ctx: Context = Depends(get_context)) -&gt; str:     user_agent = ctx.headers.get(\"user-agent\")     session_id = ctx.cookies.get(\"session_id\")     return f\"Processing {location}\"</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>@dataclass\nclass Context:\n    \"\"\"Context object providing access to request-level data in MCP tools\n\n    This class provides access to HTTP headers and cookies from the incoming request.\n    Use it as a dependency in your tool functions to access request context.\n\n    Example:\n        @mcp_router.tool()\n        def my_tool(location: str, ctx: Context = Depends(get_context)) -&gt; str:\n            user_agent = ctx.headers.get(\"user-agent\")\n            session_id = ctx.cookies.get(\"session_id\")\n            return f\"Processing {location}\"\n    \"\"\"\n\n    headers: Dict[str, str]\n    cookies: Dict[str, str]\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter","title":"<code>MCPAPIRouter</code>","text":"<p>Router for MCP JSON-RPC methods with FastAPI-like decorator API</p> <p>Inspired by FastMCP from the official MCP Python SDK: https://github.com/modelcontextprotocol/python-sdk</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>class MCPAPIRouter:\n    \"\"\"Router for MCP JSON-RPC methods with FastAPI-like decorator API\n\n    Inspired by FastMCP from the official MCP Python SDK:\n    https://github.com/modelcontextprotocol/python-sdk\n    \"\"\"\n\n    def __init__(\n        self,\n        prefix: str = \"/mcp\",\n        name: str = \"agentor-mcp-server\",\n        version: str = \"0.1.0\",\n        instructions: Optional[str] = None,\n        website_url: Optional[str] = None,\n        icons: Optional[List[Icon]] = None,\n        dependencies: Optional[List[Callable]] = None,\n    ):\n        self.prefix = prefix\n        self.name = name\n        self.version = version\n        self.instructions = instructions\n        self.website_url = website_url\n        self.icons = icons\n\n        # Storage for registered items\n        self.method_handlers: Dict[str, Callable] = {}\n        self.tools: Dict[str, ToolMetadata] = {}\n        self.resources: Dict[str, ResourceMetadata] = {}\n        self.prompts: Dict[str, PromptMetadata] = {}\n\n        self._fastapi_router = APIRouter(dependencies=dependencies)\n        self._register_default_handlers()\n        self._register_endpoint()\n\n    def _register_endpoint(self):\n        \"\"\"Register the main MCP endpoint\"\"\"\n\n        @self._fastapi_router.post(self.prefix)\n        async def mcp_handler(request: Request):\n            # Store request in context variable for tools to access\n            _request_context.set(request)\n\n            try:\n                body = await request.json()\n                method = body.get(\"method\")\n                request_id = body.get(\"id\")\n\n                logger.debug(\"Received request: %s\", body)\n\n                if method in self.method_handlers:\n                    try:\n                        result = await self.method_handlers[method](body)\n\n                        if isinstance(result, dict) and \"jsonrpc\" in result:\n                            response = result\n                        else:\n                            response = {\n                                \"jsonrpc\": \"2.0\",\n                                \"id\": request_id,\n                                \"result\": result,\n                            }\n\n                        logger.debug(\"Sending response: %s\", response)\n                        return response\n\n                    except Exception:\n                        logger.exception(\n                            \"Exception occurred processing MCP method '%s' (id=%s):\",\n                            method,\n                            request_id,\n                        )\n                        return {\n                            \"jsonrpc\": \"2.0\",\n                            \"id\": request_id,\n                            \"error\": {\n                                \"code\": -32603,\n                                \"message\": \"Internal error\",\n                            },\n                        }\n                else:\n                    return {\n                        \"jsonrpc\": \"2.0\",\n                        \"id\": request_id,\n                        \"error\": {\"code\": -32601, \"message\": \"Method not found\"},\n                    }\n            finally:\n                # Clean up context variable\n                _request_context.set(None)\n\n    def _generate_schema_from_function(self, func: Callable) -&gt; Dict[str, Any]:\n        \"\"\"Generate JSON schema from function signature\"\"\"\n        sig = inspect.signature(func)\n        type_hints = get_type_hints(func)\n\n        properties = {}\n        required = []\n\n        type_map = {\n            str: \"string\",\n            int: \"integer\",\n            float: \"number\",\n            bool: \"boolean\",\n            list: \"array\",\n            dict: \"object\",\n        }\n\n        for param_name, param in sig.parameters.items():\n            if param_name == \"self\":\n                continue\n\n            if self._extract_dependency_marker(param) is not None:\n                continue\n\n            param_type = type_hints.get(param_name, str)\n            properties[param_name] = {\n                \"type\": type_map.get(param_type, \"string\"),\n                \"description\": f\"Parameter: {param_name}\",\n            }\n\n            if param.default == inspect.Parameter.empty:\n                required.append(param_name)\n\n        schema = {\"type\": \"object\", \"properties\": properties}\n        if required:\n            schema[\"required\"] = required\n\n        return schema\n\n    def _extract_dependency_from_annotation(\n        self, annotation: Any\n    ) -&gt; Optional[FastAPIDepends]:\n        if annotation in (inspect._empty, None):\n            return None\n\n        if isinstance(annotation, FastAPIDepends):\n            return annotation\n\n        origin = get_origin(annotation)\n        if origin is Annotated:\n            metadata = get_args(annotation)[1:]\n            for meta in metadata:\n                if isinstance(meta, FastAPIDepends):\n                    return meta\n\n        return None\n\n    def _extract_dependency_marker(\n        self, param: inspect.Parameter\n    ) -&gt; Optional[FastAPIDepends]:\n        marker = self._extract_dependency_from_annotation(param.annotation)\n        if marker is not None:\n            return marker\n\n        if isinstance(param.default, FastAPIDepends):\n            return param.default\n\n        return None\n\n    def _generate_dependencies_from_function(\n        self, func: Callable\n    ) -&gt; Dict[str, Depends]:\n        \"\"\"Generate dependencies from function signature using Depends\"\"\"\n        sig = inspect.signature(func)\n        dependencies: Dict[str, FastAPIDepends] = {}\n\n        for param_name, param in sig.parameters.items():\n            dependency_marker = self._extract_dependency_marker(param)\n            if dependency_marker is not None:\n                dependencies[param_name] = dependency_marker\n\n        return dependencies\n\n    async def _resolve_dependency_marker(\n        self,\n        dependency: FastAPIDepends,\n        stack: Optional[Set[Callable]] = None,\n    ) -&gt; Any:\n        dep_callable = dependency.dependency\n        if dep_callable is None:\n            raise ValueError(\"Dependency marker is missing a callable\")\n\n        return await self._call_with_dependencies(dep_callable, stack)\n\n    async def _call_with_dependencies(\n        self,\n        func: Callable,\n        stack: Optional[Set[Callable]] = None,\n    ) -&gt; Any:\n        stack = stack or set()\n        if func in stack:\n            raise RuntimeError(\n                f\"Circular dependency detected while resolving '{func.__name__}'\"\n            )\n\n        stack.add(func)\n\n        try:\n            sig = inspect.signature(func)\n            kwargs: Dict[str, Any] = {}\n\n            for param_name, param in sig.parameters.items():\n                marker = self._extract_dependency_marker(param)\n                if marker is not None:\n                    kwargs[param_name] = await self._resolve_dependency_marker(\n                        marker, stack\n                    )\n                    continue\n\n                if param.default != inspect.Parameter.empty:\n                    continue\n\n                raise ValueError(\n                    f\"Cannot resolve parameter '{param_name}' for dependency '{func.__name__}'\"\n                )\n\n            value = func(**kwargs)\n            if inspect.isawaitable(value):\n                value = await value\n\n            return value\n        finally:\n            stack.remove(func)\n\n    async def _resolve_dependencies(\n        self, dependencies: Dict[str, FastAPIDepends]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Resolve dependency callables to concrete values\"\"\"\n\n        resolved: Dict[str, Any] = {}\n\n        for name, dependency in dependencies.items():\n            resolved[name] = await self._resolve_dependency_marker(dependency, set())\n\n        return resolved\n\n    def _register_default_handlers(self):\n        \"\"\"Register default MCP handlers\"\"\"\n\n        @self.method(\"initialize\")\n        async def default_initialize(body: dict):\n            params = body.get(\"params\", {})\n            result = InitializeResult(\n                protocolVersion=params.get(\"protocolVersion\"),\n                capabilities=ServerCapabilities(\n                    tools=ToolsCapability(listChanged=True) if self.tools else None,\n                    resources=ResourcesCapability(listChanged=True)\n                    if self.resources\n                    else None,\n                    prompts=PromptsCapability(listChanged=True)\n                    if self.prompts\n                    else None,\n                ),\n                serverInfo=Implementation(\n                    name=self.name,\n                    version=self.version,\n                    websiteUrl=self.website_url,\n                    icons=self.icons,\n                ),\n                instructions=self.instructions,\n            )\n            return result.model_dump(exclude_none=True)\n\n        @self.method(\"notifications/initialized\")\n        async def default_initialized_notification(body: dict):\n            logger.debug(\"Client initialized\")\n            return {}\n\n        @self.method(\"ping\")\n        async def default_ping(body: dict):\n            return {}\n\n        # Tool handlers\n        @self.method(\"tools/list\")\n        async def default_tools_list(body: dict):\n            return {\n                \"tools\": [\n                    {\n                        \"name\": meta.name,\n                        \"description\": meta.description,\n                        \"inputSchema\": meta.input_schema,\n                    }\n                    for meta in self.tools.values()\n                ]\n            }\n\n        @self.method(\"tools/call\")\n        async def default_tools_call(body: dict):\n            params = body.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            arguments = params.get(\"arguments\") or {}\n\n            if tool_name not in self.tools:\n                return {\n                    \"content\": [{\"type\": \"text\", \"text\": f\"Unknown tool: {tool_name}\"}],\n                    \"isError\": True,\n                }\n\n            try:\n                tool_meta = self.tools[tool_name]\n                dependencies = tool_meta.dependencies or {}\n                resolved_dependencies = (\n                    await self._resolve_dependencies(dependencies)\n                    if dependencies\n                    else {}\n                )\n                call_kwargs = {**arguments, **resolved_dependencies}\n\n                if inspect.iscoroutinefunction(tool_meta.func):\n                    result = await tool_meta.func(**call_kwargs)\n                else:\n                    result = tool_meta.func(**call_kwargs)\n\n                if isinstance(result, str):\n                    content = [{\"type\": \"text\", \"text\": result}]\n                elif isinstance(result, list):\n                    content = result\n                elif isinstance(result, dict):\n                    content = result.get(\n                        \"content\", [{\"type\": \"text\", \"text\": json.dumps(result)}]\n                    )\n                else:\n                    content = [{\"type\": \"text\", \"text\": str(result)}]\n\n                return {\"content\": content}\n\n            except Exception as e:\n                logger.exception(\"Error executing tool '%s': %s\", tool_name, str(e))\n                return {\n                    \"content\": [{\"type\": \"text\", \"text\": f\"Error: {str(e)}\"}],\n                    \"isError\": True,\n                }\n\n        # Resource handlers\n        @self.method(\"resources/list\")\n        async def default_resources_list(body: dict):\n            return {\n                \"resources\": [\n                    {\n                        \"uri\": meta.uri,\n                        \"name\": meta.name,\n                        \"description\": meta.description,\n                        \"mimeType\": meta.mime_type,\n                    }\n                    for meta in self.resources.values()\n                ]\n            }\n\n        @self.method(\"resources/read\")\n        async def default_resources_read(body: dict):\n            params = body.get(\"params\", {})\n            uri = params.get(\"uri\")\n\n            if uri not in self.resources:\n                return {\"contents\": [], \"isError\": True}\n\n            try:\n                resource_meta = self.resources[uri]\n\n                if inspect.iscoroutinefunction(resource_meta.func):\n                    result = await resource_meta.func(uri)\n                else:\n                    result = resource_meta.func(uri)\n\n                if isinstance(result, str):\n                    contents = [\n                        {\n                            \"uri\": uri,\n                            \"mimeType\": resource_meta.mime_type or \"text/plain\",\n                            \"text\": result,\n                        }\n                    ]\n                elif isinstance(result, dict):\n                    contents = [result]\n                else:\n                    contents = result\n\n                return {\"contents\": contents}\n\n            except Exception as e:\n                logger.exception(\"Error reading resource '%s': %s\", uri, str(e))\n                return {\"contents\": [], \"isError\": True}\n\n        @self.method(\"resources/templates/list\")\n        async def default_resources_templates_list(body: dict):\n            return {\"resourceTemplates\": []}\n\n        # Prompt handlers\n        @self.method(\"prompts/list\")\n        async def default_prompts_list(body: dict):\n            return {\n                \"prompts\": [\n                    {\n                        \"name\": meta.name,\n                        \"description\": meta.description,\n                        \"arguments\": meta.arguments or [],\n                    }\n                    for meta in self.prompts.values()\n                ]\n            }\n\n        @self.method(\"prompts/get\")\n        async def default_prompts_get(body: dict):\n            params = body.get(\"params\", {})\n            prompt_name = params.get(\"name\")\n            arguments = params.get(\"arguments\", {})\n\n            if prompt_name not in self.prompts:\n                return {\"messages\": [], \"isError\": True}\n\n            try:\n                prompt_meta = self.prompts[prompt_name]\n\n                if inspect.iscoroutinefunction(prompt_meta.func):\n                    result = await prompt_meta.func(**arguments)\n                else:\n                    result = prompt_meta.func(**arguments)\n\n                if isinstance(result, str):\n                    messages = [\n                        {\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": result}}\n                    ]\n                elif isinstance(result, list):\n                    messages = result\n                else:\n                    messages = [result]\n\n                return {\"description\": prompt_meta.description, \"messages\": messages}\n\n            except Exception as e:\n                logger.exception(\"Error executing prompt '%s': %s\", prompt_name, str(e))\n                return {\"messages\": [], \"isError\": True}\n\n    def tool(\n        self,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        input_schema: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Decorator to register a tool\"\"\"\n\n        def decorator(func: Callable):\n            tool_name = name or func.__name__\n            tool_description = description or (func.__doc__ or \"\").strip()\n            schema = input_schema or self._generate_schema_from_function(func)\n            dependencies = self._generate_dependencies_from_function(func)\n            self.tools[tool_name] = ToolMetadata(\n                func=func,\n                name=tool_name,\n                description=tool_description,\n                input_schema=schema,\n                dependencies=dependencies,\n            )\n            return func\n\n        return decorator\n\n    def resource(\n        self,\n        uri: str,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        mime_type: Optional[str] = None,\n    ):\n        \"\"\"Decorator to register a resource\"\"\"\n\n        def decorator(func: Callable):\n            resource_name = name or uri\n            resource_description = description or func.__doc__ or \"\"\n\n            self.resources[uri] = ResourceMetadata(\n                func=func,\n                uri=uri,\n                name=resource_name,\n                description=resource_description.strip(),\n                mime_type=mime_type,\n            )\n            return func\n\n        return decorator\n\n    def prompt(\n        self,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        arguments: Optional[List[Dict[str, Any]]] = None,\n    ):\n        \"\"\"Decorator to register a prompt\"\"\"\n\n        def decorator(func: Callable):\n            prompt_name = name or func.__name__\n            prompt_description = (\n                description or (func.__doc__ or f\"Prompt: {prompt_name}\").strip()\n            )\n\n            if arguments is None:\n                sig = inspect.signature(func)\n                args_list = [\n                    {\n                        \"name\": param_name,\n                        \"description\": f\"Parameter: {param_name}\",\n                        \"required\": param.default == inspect.Parameter.empty,\n                    }\n                    for param_name, param in sig.parameters.items()\n                    if param_name != \"self\"\n                ]\n                prompt_arguments = args_list if args_list else None\n            else:\n                prompt_arguments = arguments\n\n            self.prompts[prompt_name] = PromptMetadata(\n                func=func,\n                name=prompt_name,\n                description=prompt_description,\n                arguments=prompt_arguments,\n            )\n            return func\n\n        return decorator\n\n    def method(self, method_name: str):\n        \"\"\"Decorator to register custom MCP method handlers\"\"\"\n\n        def decorator(func: Callable):\n            self.method_handlers[method_name] = func\n            return func\n\n        return decorator\n\n    def get_fastapi_router(self) -&gt; APIRouter:\n        \"\"\"Get the underlying FastAPI router\"\"\"\n        return self._fastapi_router\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter-functions","title":"Functions","text":""},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter.tool","title":"<code>tool(name=None, description=None, input_schema=None)</code>","text":"<p>Decorator to register a tool</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def tool(\n    self,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    input_schema: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Decorator to register a tool\"\"\"\n\n    def decorator(func: Callable):\n        tool_name = name or func.__name__\n        tool_description = description or (func.__doc__ or \"\").strip()\n        schema = input_schema or self._generate_schema_from_function(func)\n        dependencies = self._generate_dependencies_from_function(func)\n        self.tools[tool_name] = ToolMetadata(\n            func=func,\n            name=tool_name,\n            description=tool_description,\n            input_schema=schema,\n            dependencies=dependencies,\n        )\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter.resource","title":"<code>resource(uri, name=None, description=None, mime_type=None)</code>","text":"<p>Decorator to register a resource</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def resource(\n    self,\n    uri: str,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    mime_type: Optional[str] = None,\n):\n    \"\"\"Decorator to register a resource\"\"\"\n\n    def decorator(func: Callable):\n        resource_name = name or uri\n        resource_description = description or func.__doc__ or \"\"\n\n        self.resources[uri] = ResourceMetadata(\n            func=func,\n            uri=uri,\n            name=resource_name,\n            description=resource_description.strip(),\n            mime_type=mime_type,\n        )\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter.prompt","title":"<code>prompt(name=None, description=None, arguments=None)</code>","text":"<p>Decorator to register a prompt</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def prompt(\n    self,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    arguments: Optional[List[Dict[str, Any]]] = None,\n):\n    \"\"\"Decorator to register a prompt\"\"\"\n\n    def decorator(func: Callable):\n        prompt_name = name or func.__name__\n        prompt_description = (\n            description or (func.__doc__ or f\"Prompt: {prompt_name}\").strip()\n        )\n\n        if arguments is None:\n            sig = inspect.signature(func)\n            args_list = [\n                {\n                    \"name\": param_name,\n                    \"description\": f\"Parameter: {param_name}\",\n                    \"required\": param.default == inspect.Parameter.empty,\n                }\n                for param_name, param in sig.parameters.items()\n                if param_name != \"self\"\n            ]\n            prompt_arguments = args_list if args_list else None\n        else:\n            prompt_arguments = arguments\n\n        self.prompts[prompt_name] = PromptMetadata(\n            func=func,\n            name=prompt_name,\n            description=prompt_description,\n            arguments=prompt_arguments,\n        )\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter.method","title":"<code>method(method_name)</code>","text":"<p>Decorator to register custom MCP method handlers</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def method(self, method_name: str):\n    \"\"\"Decorator to register custom MCP method handlers\"\"\"\n\n    def decorator(func: Callable):\n        self.method_handlers[method_name] = func\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.MCPAPIRouter.get_fastapi_router","title":"<code>get_fastapi_router()</code>","text":"<p>Get the underlying FastAPI router</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def get_fastapi_router(self) -&gt; APIRouter:\n    \"\"\"Get the underlying FastAPI router\"\"\"\n    return self._fastapi_router\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router-functions","title":"Functions","text":""},{"location":"api/mcp/api_router/#agentor.mcp.api_router.get_context","title":"<code>get_context()</code>","text":"<p>Dependency function to retrieve the current request context</p> <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>A Context object with headers and cookies from the current request</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside of a request context</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def get_context() -&gt; Context:\n    \"\"\"Dependency function to retrieve the current request context\n\n    Returns:\n        Context: A Context object with headers and cookies from the current request\n\n    Raises:\n        RuntimeError: If called outside of a request context\n    \"\"\"\n    request = _request_context.get()\n    if request is None:\n        # Return empty context if no request is available\n        return Context(headers={}, cookies={})\n\n    # Convert Headers to dict for easier access\n    headers = dict(request.headers)\n    cookies = dict(request.cookies)\n\n    return Context(headers=headers, cookies=cookies)\n</code></pre>"},{"location":"api/mcp/api_router/#agentor.mcp.api_router.get_token","title":"<code>get_token()</code>","text":"<p>Get the token from the request headers. Returns:     The token from the request headers. Doesn't include the \"Bearer \" prefix.     Returns None if no token is found.</p> Source code in <code>src/agentor/mcp/api_router.py</code> <pre><code>def get_token() -&gt; str:\n    \"\"\"Get the token from the request headers.\n    Returns:\n        The token from the request headers. Doesn't include the \"Bearer \" prefix.\n        Returns None if no token is found.\n    \"\"\"\n    headers = get_headers()\n    auth = headers.get(\"Authorization\", None) or headers.get(\"authorization\", None)\n    if auth is None:\n        return None\n    return auth.split(\" \")[1]\n</code></pre>"},{"location":"api/mcp/proxy/","title":"Proxy","text":""},{"location":"api/mcp/proxy/#agentor.mcp.proxy","title":"<code>proxy</code>","text":""},{"location":"api/mcp/proxy/#agentor.mcp.proxy-functions","title":"Functions","text":""},{"location":"api/mcp/proxy/#agentor.mcp.proxy.create_proxy","title":"<code>create_proxy(remote_url, name=None)</code>","text":"<p>Create a proxy to a remote MCP server.</p> Example <p>agentor create-proxy https://example.com/mcp/ MyProxyServer</p> <p>Parameters:</p> Name Type Description Default <code>remote_url</code> <code>str</code> <p>The URL of the remote MCP server.</p> required <code>name</code> <code>str | None</code> <p>The name of the proxy. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FastMCP</code> <p>The proxy server.</p> Source code in <code>src/agentor/mcp/proxy.py</code> <pre><code>@app.command()\ndef create_proxy(remote_url: str, name: str | None = None):\n    \"\"\"Create a proxy to a remote MCP server.\n\n    Example:\n        agentor create-proxy https://example.com/mcp/ MyProxyServer\n\n    Args:\n        remote_url (str): The URL of the remote MCP server.\n        name (str | None, optional): The name of the proxy. Defaults to None.\n\n    Returns:\n        FastMCP: The proxy server.\n    \"\"\"\n    try:\n        from fastmcp import FastMCP\n\n        mcp_server = FastMCP.as_proxy(remote_url, name=name)\n        mcp_server.run()\n    except Exception as e:\n        typer.echo(f\"Error creating proxy: {e}\", err=True)\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/mcp/server/","title":"Server","text":""},{"location":"api/mcp/server/#agentor.mcp.server","title":"<code>server</code>","text":""},{"location":"api/mcp/server/#agentor.mcp.server-classes","title":"Classes","text":""},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP","title":"<code>LiteMCP</code>","text":"<p>               Bases: <code>MCPAPIRouter</code></p> <p>ASGI-compatible MCP server built on FastAPI</p> <p>This class can be used directly as an ASGI application or run with uvicorn.</p> Example Source code in <code>src/agentor/mcp/server.py</code> <pre><code>class LiteMCP(MCPAPIRouter):\n    \"\"\"ASGI-compatible MCP server built on FastAPI\n\n    This class can be used directly as an ASGI application or run with uvicorn.\n\n    Example:\n        # As ASGI app\n        app = LiteMCP()\n\n        # Run with uvicorn\n        app.run()\n\n        # Or use with uvicorn CLI\n        # uvicorn module:app\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"Initialize LiteMCP server\n\n        Args:\n            **kwargs: Additional arguments passed to MCPAPIRouter\n        \"\"\"\n        super().__init__(**kwargs)\n\n        # Create FastAPI app\n        self.app = FastAPI(\n            title=self.name,\n            version=self.version,\n            description=self.instructions,\n        )\n        # Include the MCP router\n        self.app.include_router(self._fastapi_router)\n\n    async def __call__(self, scope: dict, receive: Any, send: Any) -&gt; None:\n        \"\"\"ASGI interface - delegates to FastAPI app\n\n        This makes LiteMCP a proper ASGI application that can be used with\n        any ASGI server (uvicorn, hypercorn, daphne, etc.)\n        \"\"\"\n        await self.app(scope, receive, send)\n\n    def serve(\n        self,\n        host: str = \"0.0.0.0\",\n        port: int = 8000,\n        enable_cors: bool = True,\n        **uvicorn_kwargs,\n    ):\n        \"\"\"Run the server with uvicorn\n\n        Args:\n            **uvicorn_kwargs: Additional arguments passed to uvicorn.run()\n        \"\"\"\n        if enable_cors:\n            self.app.add_middleware(\n                CORSMiddleware,\n                allow_origins=[\"*\"],\n                allow_credentials=True,\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n        uvicorn_config = {\"host\": host, \"port\": port, **uvicorn_kwargs}\n        print_rich(f\"Running MCP server at http://{host}:{port}{self.prefix}\")\n        uvicorn.run(self.app, **uvicorn_config)\n\n    def run(self, *args, **kwargs):\n        \"\"\"Run the MCP server using uvicorn\"\"\"\n        logger.warning(\"This method is deprecated. Use serve() instead.\")\n        return self.serve(*args, **kwargs)\n</code></pre>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP--as-asgi-app","title":"As ASGI app","text":"<p>app = LiteMCP()</p>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP--run-with-uvicorn","title":"Run with uvicorn","text":"<p>app.run()</p>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP--or-use-with-uvicorn-cli","title":"Or use with uvicorn CLI","text":""},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP--uvicorn-moduleapp","title":"uvicorn module:app","text":""},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP-functions","title":"Functions","text":""},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize LiteMCP server</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional arguments passed to MCPAPIRouter</p> <code>{}</code> Source code in <code>src/agentor/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    \"\"\"Initialize LiteMCP server\n\n    Args:\n        **kwargs: Additional arguments passed to MCPAPIRouter\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # Create FastAPI app\n    self.app = FastAPI(\n        title=self.name,\n        version=self.version,\n        description=self.instructions,\n    )\n    # Include the MCP router\n    self.app.include_router(self._fastapi_router)\n</code></pre>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP.__call__","title":"<code>__call__(scope, receive, send)</code>  <code>async</code>","text":"<p>ASGI interface - delegates to FastAPI app</p> <p>This makes LiteMCP a proper ASGI application that can be used with any ASGI server (uvicorn, hypercorn, daphne, etc.)</p> Source code in <code>src/agentor/mcp/server.py</code> <pre><code>async def __call__(self, scope: dict, receive: Any, send: Any) -&gt; None:\n    \"\"\"ASGI interface - delegates to FastAPI app\n\n    This makes LiteMCP a proper ASGI application that can be used with\n    any ASGI server (uvicorn, hypercorn, daphne, etc.)\n    \"\"\"\n    await self.app(scope, receive, send)\n</code></pre>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP.serve","title":"<code>serve(host='0.0.0.0', port=8000, enable_cors=True, **uvicorn_kwargs)</code>","text":"<p>Run the server with uvicorn</p> <p>Parameters:</p> Name Type Description Default <code>**uvicorn_kwargs</code> <p>Additional arguments passed to uvicorn.run()</p> <code>{}</code> Source code in <code>src/agentor/mcp/server.py</code> <pre><code>def serve(\n    self,\n    host: str = \"0.0.0.0\",\n    port: int = 8000,\n    enable_cors: bool = True,\n    **uvicorn_kwargs,\n):\n    \"\"\"Run the server with uvicorn\n\n    Args:\n        **uvicorn_kwargs: Additional arguments passed to uvicorn.run()\n    \"\"\"\n    if enable_cors:\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n    uvicorn_config = {\"host\": host, \"port\": port, **uvicorn_kwargs}\n    print_rich(f\"Running MCP server at http://{host}:{port}{self.prefix}\")\n    uvicorn.run(self.app, **uvicorn_config)\n</code></pre>"},{"location":"api/mcp/server/#agentor.mcp.server.LiteMCP.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Run the MCP server using uvicorn</p> Source code in <code>src/agentor/mcp/server.py</code> <pre><code>def run(self, *args, **kwargs):\n    \"\"\"Run the MCP server using uvicorn\"\"\"\n    logger.warning(\"This method is deprecated. Use serve() instead.\")\n    return self.serve(*args, **kwargs)\n</code></pre>"},{"location":"api/tools/base/","title":"Base","text":""},{"location":"api/tools/base/#agentor.tools.base","title":"<code>base</code>","text":""},{"location":"api/tools/base/#agentor.tools.base-classes","title":"Classes","text":""},{"location":"api/tools/base/#agentor.tools.base.BaseTool","title":"<code>BaseTool</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all tools in Agentor. Supports both local execution and MCP serving.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>class BaseTool(ABC):\n    \"\"\"\n    Base class for all tools in Agentor.\n    Supports both local execution and MCP serving.\n    \"\"\"\n\n    name: str = \"un-named-tool\"\n    description: str | None = None\n\n    def __init__(self, api_key: Optional[str] = None):\n        self.api_key = api_key\n        self._mcp_server: Optional[LiteMCP] = None\n\n    def list_capabilities(self) -&gt; List[Tuple[str, FunctionType]]:\n        \"\"\"List all capabilities of the tool.\"\"\"\n        return [\n            (attr, getattr(self, attr))\n            for attr in dir(self)\n            if getattr(getattr(self, attr), \"_is_capability\", False) is True\n        ]\n\n    def _get_tool(self, name: str) -&gt; Callable:\n        \"\"\"Get a function marked as a tool capability.\"\"\"\n        func = getattr(self, name)\n        if getattr(func, \"_is_capability\", False) is not True:\n            raise ValueError(f\"Tool '{name}' doesn't exist for the class '{self.name}'\")\n        return func\n\n    def to_openai_function(self) -&gt; List[FunctionTool]:\n        \"\"\"Convert all capabilities to OpenAI-compatible FunctionTools.\"\"\"\n        tools = []\n\n        # Check for capabilities\n        for attr_name in dir(self):\n            attr = getattr(self, attr_name)\n            if getattr(attr, \"_is_capability\", False) is True:\n                tools.append(function_tool(attr, strict_mode=False))\n\n        return tools\n\n    @cache\n    def json_schema(self) -&gt; List[ToolType]:\n        \"\"\"Convert all capabilities to JSON Schema.\"\"\"\n        function_tools = self.to_openai_function()\n        result: list[ToolType] = []\n        for func_tool in function_tools:\n            item = {\n                \"type\": \"function\",\n                \"name\": self.name,\n                \"description\": self.description,\n                \"parameters\": func_tool.params_json_schema,\n            }\n            result.append(item)\n        return result\n\n    def serve(self, name: Optional[str] = None, port: int = 8000):\n        \"\"\"Serve the tool as an MCP server using LiteMCP.\"\"\"\n        server_name = name or self.name\n        self._mcp_server = LiteMCP(name=server_name, version=\"1.0.0\")\n\n        # Register all capabilities with LiteMCP\n        for attr_name in dir(self):\n            func = getattr(self, attr_name)\n            if getattr(func, \"_is_capability\", False) is True:\n                self._mcp_server.tool(name=func.__name__, description=func.__doc__)(\n                    func\n                )\n\n        # LiteMCP run method handles starting the server\n        self._mcp_server.run(port=port)\n\n    @overload\n    def run(self, *args, **kwargs) -&gt; Optional[str]: ...\n\n    @overload\n    def run(self, request: Any) -&gt; Optional[str]: ...\n\n    def run(self, *args, **kwargs) -&gt; Optional[str]:\n        raise NotImplementedError(\n            \"This method is dynamically registered using the BaseTool.from_function method.\"\n        )\n\n    @staticmethod\n    def from_function(\n        func: Callable, name: str | None = None, description: str | None = None\n    ) -&gt; \"BaseTool\":\n        \"\"\"Register a function as a tool capability and access using the run method.\n\n        Args:\n            func: The function to be registered.\n\n        Example:\n            &gt;&gt;&gt; from agentor.tools.base import BaseTool\n            &gt;&gt;&gt; def weather_tool(city: str):\n            &gt;&gt;&gt;    \"This function returns the weather of the city.\"\n            &gt;&gt;&gt;    return f\"Weather in {city} is warm and sunny.\"\n            &gt;&gt;&gt; tool = BaseTool.from_function(weather_tool)\n            &gt;&gt;&gt; tool.run(\"London\")  # Output: Weather in London is warm and sunny.\n        \"\"\"\n\n        tool_name = name or func.__name__\n        tool_description = description or func.__doc__\n\n        class NewDynamicTool(BaseTool):\n            name = tool_name\n            description = tool_description\n\n            @capability\n            def run(self, *args, **kwargs) -&gt; Optional[str]:\n                return func(*args, **kwargs)\n\n        return NewDynamicTool()\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base.BaseTool-functions","title":"Functions","text":""},{"location":"api/tools/base/#agentor.tools.base.BaseTool.list_capabilities","title":"<code>list_capabilities()</code>","text":"<p>List all capabilities of the tool.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>def list_capabilities(self) -&gt; List[Tuple[str, FunctionType]]:\n    \"\"\"List all capabilities of the tool.\"\"\"\n    return [\n        (attr, getattr(self, attr))\n        for attr in dir(self)\n        if getattr(getattr(self, attr), \"_is_capability\", False) is True\n    ]\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base.BaseTool.to_openai_function","title":"<code>to_openai_function()</code>","text":"<p>Convert all capabilities to OpenAI-compatible FunctionTools.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>def to_openai_function(self) -&gt; List[FunctionTool]:\n    \"\"\"Convert all capabilities to OpenAI-compatible FunctionTools.\"\"\"\n    tools = []\n\n    # Check for capabilities\n    for attr_name in dir(self):\n        attr = getattr(self, attr_name)\n        if getattr(attr, \"_is_capability\", False) is True:\n            tools.append(function_tool(attr, strict_mode=False))\n\n    return tools\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base.BaseTool.json_schema","title":"<code>json_schema()</code>  <code>cached</code>","text":"<p>Convert all capabilities to JSON Schema.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>@cache\ndef json_schema(self) -&gt; List[ToolType]:\n    \"\"\"Convert all capabilities to JSON Schema.\"\"\"\n    function_tools = self.to_openai_function()\n    result: list[ToolType] = []\n    for func_tool in function_tools:\n        item = {\n            \"type\": \"function\",\n            \"name\": self.name,\n            \"description\": self.description,\n            \"parameters\": func_tool.params_json_schema,\n        }\n        result.append(item)\n    return result\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base.BaseTool.serve","title":"<code>serve(name=None, port=8000)</code>","text":"<p>Serve the tool as an MCP server using LiteMCP.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>def serve(self, name: Optional[str] = None, port: int = 8000):\n    \"\"\"Serve the tool as an MCP server using LiteMCP.\"\"\"\n    server_name = name or self.name\n    self._mcp_server = LiteMCP(name=server_name, version=\"1.0.0\")\n\n    # Register all capabilities with LiteMCP\n    for attr_name in dir(self):\n        func = getattr(self, attr_name)\n        if getattr(func, \"_is_capability\", False) is True:\n            self._mcp_server.tool(name=func.__name__, description=func.__doc__)(\n                func\n            )\n\n    # LiteMCP run method handles starting the server\n    self._mcp_server.run(port=port)\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base.BaseTool.from_function","title":"<code>from_function(func, name=None, description=None)</code>  <code>staticmethod</code>","text":"<p>Register a function as a tool capability and access using the run method.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be registered.</p> required Example <p>from agentor.tools.base import BaseTool def weather_tool(city: str):    \"This function returns the weather of the city.\"    return f\"Weather in {city} is warm and sunny.\" tool = BaseTool.from_function(weather_tool) tool.run(\"London\")  # Output: Weather in London is warm and sunny.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>@staticmethod\ndef from_function(\n    func: Callable, name: str | None = None, description: str | None = None\n) -&gt; \"BaseTool\":\n    \"\"\"Register a function as a tool capability and access using the run method.\n\n    Args:\n        func: The function to be registered.\n\n    Example:\n        &gt;&gt;&gt; from agentor.tools.base import BaseTool\n        &gt;&gt;&gt; def weather_tool(city: str):\n        &gt;&gt;&gt;    \"This function returns the weather of the city.\"\n        &gt;&gt;&gt;    return f\"Weather in {city} is warm and sunny.\"\n        &gt;&gt;&gt; tool = BaseTool.from_function(weather_tool)\n        &gt;&gt;&gt; tool.run(\"London\")  # Output: Weather in London is warm and sunny.\n    \"\"\"\n\n    tool_name = name or func.__name__\n    tool_description = description or func.__doc__\n\n    class NewDynamicTool(BaseTool):\n        name = tool_name\n        description = tool_description\n\n        @capability\n        def run(self, *args, **kwargs) -&gt; Optional[str]:\n            return func(*args, **kwargs)\n\n    return NewDynamicTool()\n</code></pre>"},{"location":"api/tools/base/#agentor.tools.base-functions","title":"Functions","text":""},{"location":"api/tools/base/#agentor.tools.base.capability","title":"<code>capability(func)</code>","text":"<p>Decorator to mark a method as a tool capability.</p> Source code in <code>src/agentor/tools/base.py</code> <pre><code>def capability(func: Callable):\n    \"\"\"Decorator to mark a method as a tool capability.\"\"\"\n    func._is_capability = True\n    return func\n</code></pre>"},{"location":"api/tools/registry/","title":"Registry","text":""},{"location":"api/tools/registry/#agentor.tools.registry","title":"<code>registry</code>","text":""},{"location":"api/tools/registry/#agentor.tools.registry-classes","title":"Classes","text":""},{"location":"api/tools/registry/#agentor.tools.registry.ToolRegistry","title":"<code>ToolRegistry</code>","text":"<p>Registry for tools.</p> Source code in <code>src/agentor/tools/registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Registry for tools.\"\"\"\n\n    @staticmethod\n    def get(name: str) -&gt; Union[FunctionTool, Callable]:\n        try:\n            return _GLOBAL_TOOLS[name]\n        except KeyError:\n            raise ValueError(f\"Tool {name} not found\")\n\n    @staticmethod\n    def list() -&gt; List[str]:\n        return tuple(_GLOBAL_TOOLS.keys())\n\n    @staticmethod\n    def __len__() -&gt; int:\n        return len(_GLOBAL_TOOLS)\n</code></pre>"},{"location":"api/tools/registry/#agentor.tools.registry-functions","title":"Functions","text":""},{"location":"api/tools/registry/#agentor.tools.registry.get_weather","title":"<code>get_weather(wrapper, city)</code>","text":"<p>Returns the weather in the given city.</p> Source code in <code>src/agentor/tools/registry.py</code> <pre><code>@register_global_tool\ndef get_weather(wrapper: RunContextWrapper[CelestoConfig], city: str) -&gt; str:\n    \"\"\"Returns the weather in the given city.\"\"\"\n    try:\n        weather_tool = GetWeatherTool(api_key=wrapper.context.weather_api_key)\n        return weather_tool.get_current_weather(city)\n    except Exception as e:\n        return f\"Failed to get weather data: {e}\"\n</code></pre>"},{"location":"api/tools/registry/#agentor.tools.registry.current_datetime","title":"<code>current_datetime(wrapper)</code>","text":"<p>Returns the current date and time.</p> Source code in <code>src/agentor/tools/registry.py</code> <pre><code>@register_global_tool\ndef current_datetime(wrapper: RunContextWrapper[CelestoConfig]) -&gt; str:\n    \"\"\"Returns the current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api/utils/output_text_formatter/","title":"Output Text Formatter","text":""},{"location":"api/utils/output_text_formatter/#agentor.output_text_formatter","title":"<code>output_text_formatter</code>","text":""},{"location":"api/utils/output_text_formatter/#agentor.output_text_formatter-functions","title":"Functions","text":""},{"location":"api/utils/output_text_formatter/#agentor.output_text_formatter.format_stream_events","title":"<code>format_stream_events(events, allowed_events=None)</code>  <code>async</code>","text":"<p>Formats a stream of events into a stream of AgentOutput objects.</p> <ul> <li>raw_response_event: Events directly from the OpenAI Response API.</li> <li>agent_updated_stream_event: Events from the Agent updated stream.</li> <li>run_item_stream_event: Events from the Run Item stream.</li> </ul> Source code in <code>src/agentor/output_text_formatter.py</code> <pre><code>async def format_stream_events(\n    events: AsyncIterator[StreamEvent],\n    allowed_events: Optional[List[AllowedEventTypes]] = None,\n) -&gt; AsyncIterator[AgentOutput]:\n    \"\"\"\n    Formats a stream of events into a stream of AgentOutput objects.\n\n    - raw_response_event: Events directly from the OpenAI Response API.\n    - agent_updated_stream_event: Events from the Agent updated stream.\n    - run_item_stream_event: Events from the Run Item stream.\n    \"\"\"\n    async for event in events:\n        stream_event = format_event(event)\n\n        if allowed_events is not None:\n            if stream_event.type not in allowed_events:\n                continue\n\n        if stream_event.type == \"agent_updated_stream_event\":\n            yield AgentOutput(\n                type=\"agent_updated_stream_event\",\n                message=stream_event.new_agent.name,\n            )\n\n        elif stream_event.type == \"raw_response_event\":\n            data = stream_event.data\n            if isinstance(data, ResponseTextDeltaEvent):\n                chunk_text = data.delta or \"\"\n                yield AgentOutput(\n                    type=\"raw_response_event\",\n                    chunk=chunk_text,\n                    raw_event=stream_event,\n                )\n            else:\n                yield AgentOutput(\n                    type=\"raw_response_event\",\n                    raw_event=stream_event,\n                )\n\n        elif stream_event.type == \"run_item_stream_event\":\n            item = stream_event.item\n            item_type = getattr(item, \"type\", None)\n\n            if item_type == \"message_output_item\":\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    message=ItemHelpers.text_message_output(item).strip(),\n                )\n            elif item_type == \"tool_call_item\":\n                tool_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=tool_name or \"tool_call_item\", type=\"tool_called\"\n                    ),\n                )\n            elif item_type == \"tool_call_output_item\":\n                tool_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                output_text = _stringify_output(getattr(item, \"output\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    message=output_text,\n                    tool_action=ToolAction(\n                        name=tool_name or \"tool_call_output_item\", type=\"tool_output\"\n                    ),\n                )\n            elif item_type == \"reasoning_item\":\n                reasoning_text = getattr(getattr(item, \"raw_item\", None), \"content\", \"\")\n                if reasoning_text is None:\n                    reasoning_text = \"\"\n                else:\n                    reasoning_text = str(reasoning_text)\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    reasoning=reasoning_text,\n                )\n            elif item_type == \"handoff_call_item\":\n                target_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=target_name or \"handoff_request\",\n                        type=\"handoff_requested\",\n                    ),\n                )\n            elif item_type == \"handoff_output_item\":\n                source_agent = getattr(getattr(item, \"source_agent\", None), \"name\", \"\")\n                target_agent = getattr(getattr(item, \"target_agent\", None), \"name\", \"\")\n                action_name = \" -&gt; \".join(\n                    part for part in (source_agent, target_agent) if part\n                )\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=action_name or \"handoff\",\n                        type=\"handoff_occured\",\n                    ),\n                )\n            elif item_type == \"mcp_approval_request_item\":\n                request_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=request_name or \"mcp_approval_request\",\n                        type=\"mcp_approval_requested\",\n                    ),\n                )\n            elif item_type == \"mcp_approval_response_item\":\n                response_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=response_name or \"mcp_approval_response\",\n                        type=\"mcp_approval_response\",\n                    ),\n                )\n            elif item_type == \"mcp_list_tools_item\":\n                list_tools_name = _extract_tool_name(getattr(item, \"raw_item\", None))\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    tool_action=ToolAction(\n                        name=list_tools_name or \"mcp_list_tools\",\n                        type=\"mcp_list_tools\",\n                    ),\n                )\n            else:\n                yield AgentOutput(\n                    type=\"run_item_stream_event\",\n                    message=f\"Unhandled run item type: {item_type or stream_event.name}\",\n                )\n\n        else:\n            raise ValueError(f\"Invalid event type: {stream_event.type}\")\n</code></pre>"},{"location":"api/utils/type_helper/","title":"Type Helper","text":""},{"location":"api/utils/type_helper/#agentor.type_helper","title":"<code>type_helper</code>","text":""},{"location":"api/utils/type_helper/#agentor.type_helper-functions","title":"Functions","text":""},{"location":"api/utils/type_helper/#agentor.type_helper.serialize","title":"<code>serialize(obj, *, convert_keys_to_str=True, decimal_to_str=True, max_depth=None, dump_json=False)</code>","text":"<p>Recursively convert <code>obj</code> into JSON-serializable Python primitives.</p> <ul> <li>Dataclasses -&gt; dict</li> <li>Pydantic BaseModel (v1/v2) -&gt; dict</li> <li>Enums -&gt; value</li> <li>datetime/date/time -&gt; ISO 8601 string</li> <li>Decimal -&gt; str (or float if decimal_to_str=False)</li> <li>UUID/Path -&gt; str</li> <li>sets/tuples -&gt; list</li> <li>numpy scalars/arrays -&gt; python scalars/lists</li> <li>Mappings/Iterables -&gt; dict/list</li> <li>Falls back to dict when sensible</li> </ul> <p>Parameters:</p> Name Type Description Default <code>convert_keys_to_str</code> <code>bool</code> <p>If True, mapping keys are stringified (JSON requires string keys).</p> <code>True</code> <code>decimal_to_str</code> <code>bool</code> <p>If True, Decimal -&gt; str, else float.</p> <code>True</code> <code>max_depth</code> <code>int | None</code> <p>Optional recursion cap; when reached, returns a stub string.</p> <code>None</code> <code>dump_json</code> <code>bool</code> <p>If True, dump the object to a JSON string.</p> <code>False</code> <p>Returns:     A structure of only {dict, list, str, int, float, bool, None} suitable for json.dumps.     If dump_json is True, returns a JSON string.     Otherwise, returns the object.</p> Source code in <code>src/agentor/type_helper.py</code> <pre><code>def serialize(\n    obj: Any,\n    *,\n    convert_keys_to_str: bool = True,\n    decimal_to_str: bool = True,\n    max_depth: int | None = None,\n    dump_json: bool = False,\n) -&gt; Any | str | bytes:\n    \"\"\"\n    Recursively convert `obj` into JSON-serializable Python primitives.\n\n    - Dataclasses -&gt; dict\n    - Pydantic BaseModel (v1/v2) -&gt; dict\n    - Enums -&gt; value\n    - datetime/date/time -&gt; ISO 8601 string\n    - Decimal -&gt; str (or float if decimal_to_str=False)\n    - UUID/Path -&gt; str\n    - sets/tuples -&gt; list\n    - numpy scalars/arrays -&gt; python scalars/lists\n    - Mappings/Iterables -&gt; dict/list\n    - Falls back to __dict__ when sensible\n\n    Args:\n        convert_keys_to_str: If True, mapping keys are stringified (JSON requires string keys).\n        decimal_to_str: If True, Decimal -&gt; str, else float.\n        max_depth: Optional recursion cap; when reached, returns a stub string.\n        dump_json: If True, dump the object to a JSON string.\n    Returns:\n        A structure of only {dict, list, str, int, float, bool, None} suitable for json.dumps.\n        If dump_json is True, returns a JSON string.\n        Otherwise, returns the object.\n    \"\"\"\n    seen: set[int] = set()\n\n    def _numpy_scalar(x: Any) -&gt; bool:\n        return np is not None and isinstance(\n            x, getattr(np, \"generic\", ())\n        )  # numpy scalar types\n\n    def _convert(o: Any, depth: int) -&gt; Any:\n        if max_depth is not None and depth &gt; max_depth:\n            return f\"&lt;max_depth reached at {type(o).__name__}&gt;\"\n\n        oid = id(o)\n        if isinstance(o, (dict, list, tuple, set)) or dataclasses.is_dataclass(o):\n            # track containers for cycle detection\n            if oid in seen:\n                return f\"&lt;recursion {type(o).__name__}&gt;\"\n            seen.add(oid)\n\n        # Primitives pass through\n        if o is None or isinstance(o, (bool, int, float, str)):\n            return o\n\n        # Dataclasses\n        if dataclasses.is_dataclass(o):\n            o = dataclasses.asdict(o)\n            return _convert(o, depth + 1)\n\n        # Pydantic models (v2 has .model_dump, v1 has .dict)\n        if isinstance(o, BaseModel):\n            try:\n                data = o.model_dump(mode=\"python\")  # pydantic v2\n            except Exception:\n                data = o.dict()  # pydantic v1\n            return _convert(data, depth + 1)\n\n        # Enums -&gt; underlying value\n        if isinstance(o, enum.Enum):\n            return _convert(o.value, depth + 1)\n\n        # datetime/date/time -&gt; ISO\n        if isinstance(o, (dt.datetime, dt.date, dt.time)):\n            # Preserve timezone info if present\n            try:\n                return o.isoformat()\n            except Exception:\n                return str(o)\n\n        # Decimal\n        if isinstance(o, Decimal):\n            return str(o) if decimal_to_str else float(o)\n\n        # UUID / Path\n        if isinstance(o, (UUID, Path)):\n            return str(o)\n\n        # bytes / bytearray -&gt; hex string (safe, no encoding assumption)\n        if isinstance(o, (bytes, bytearray, memoryview)):\n            return bytes(o).hex()\n\n        # NumPy types\n        if np is not None:\n            if isinstance(o, np.ndarray):\n                return _convert(o.tolist(), depth + 1)\n            if _numpy_scalar(o):\n                return o.item()\n\n        # Mapping\n        if isinstance(o, Mapping):\n            if convert_keys_to_str:\n                return {str(k): _convert(v, depth + 1) for k, v in o.items()}\n            else:\n                return {k: _convert(v, depth + 1) for k, v in o.items()}\n\n        # Iterables (list/tuple/set/tuple-like)\n        if isinstance(o, (list, tuple, set, frozenset)):\n            return [_convert(i, depth + 1) for i in o]\n\n        if isinstance(o, Iterable) and not isinstance(o, (str, bytes, bytearray)):\n            # Catch-all for other iterables/generators\n            return [_convert(i, depth + 1) for i in o]\n\n        # Objects with __dict__ (as a last resort)\n        if hasattr(o, \"__dict__\"):\n            return _convert(vars(o), depth + 1)\n\n        # Fallback: string representation\n        return str(o)\n\n    if dump_json:\n        return json.dumps(_convert(obj, depth=0))\n    else:\n        return _convert(obj, depth=0)\n</code></pre>"},{"location":"api/utils/utils/","title":"Utils","text":""},{"location":"api/utils/utils/#agentor.utils","title":"<code>utils</code>","text":""}]}